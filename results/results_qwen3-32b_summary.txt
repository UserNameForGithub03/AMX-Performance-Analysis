Elapsed Time: 6408.117s
    Clockticks: 30,644,082,000,000
    Instructions Retired: 196,026,196,500,000
    CPI Rate: 0.156
    Bottlenecks View (preview)
        Mispredictions
        Big Code: 0.1% of Pipeline Slots
        Instruction Fetch Bandwidth
        Cache Memory Bandwidth: 51.1% of Pipeline Slots
         | Improve this bottleneck metric by examining these TMA metrics: FB Full, SQ Full, and Memory Bandwidth
         |
        Cache Memory Latency: 7.9% of Pipeline Slots
        Memory Data TLBs: 0.2% of Pipeline Slots
        Memory Synchronization: 0.6% of Pipeline Slots
        Branching Overhead: 0.8% of Pipeline Slots
        Useful Work: 4.1% of Pipeline Slots
    Top-down Microarchitecture Analysis (TMA)
        Retiring: 4.9% of Pipeline Slots
            Light Operations: 1.1% of Pipeline Slots
                FP Arithmetic: 0.0% of uOps
                    FP x87: 0.0% of uOps
                    FP Scalar: 0.0% of uOps
                    FP Vector: 0.0% of uOps
                        128-bit FP Vector: 0.0% of uOps
                        256-bit FP Vector: 0.0% of uOps
                        512-bit FP Vector: 0.0% of uOps
                Integer Operations: 0.0% of uOps
                    128-bit Integer Vector Operations: 0.0% of uOps
                    256-bit Vector Operations: 0.0% of uOps
                Memory Operations: 0.1% of Pipeline Slots
                Fused Instructions: 0.2% of Pipeline Slots
                Non Fused Branches: 0.0% of Pipeline Slots
                Other: 0.7% of Pipeline Slots
                    Nop Instructions: 0.0% of Pipeline Slots
                    Shuffles_256b: 0.0% of Pipeline Slots
            Heavy Operations: 3.8% of Pipeline Slots
                Few Uops Instructions: 1.5% of Pipeline Slots
                Microcode Sequencer: 2.3% of Pipeline Slots
                    Assists: 0.0% of Pipeline Slots
                        Page Faults: 0.0% of Pipeline Slots
                        FP Assists: 0.0% of Pipeline Slots
                        AVX Assists: 0.0% of Pipeline Slots
                    CISC: 2.3% of Pipeline Slots
        Front-End Bound: 3.6% of Pipeline Slots
            Front-End Latency: 3.6% of Pipeline Slots
                ICache Misses: 0.2% of Clockticks
                ITLB Misses: 0.5% of Clockticks
                Branch Resteers: 0.1% of Clockticks
                    Mispredicts Resteers
                    Clears Resteers
                    Unknown Branches: 0.0% of Clockticks
                MS Switches: 18.3% of Clockticks
                Length Changing Prefixes: 0.0% of Clockticks
                DSB Switches: 0.1% of Clockticks
            Front-End Bandwidth: 0.0% of Pipeline Slots
                Front-End Bandwidth MITE: 0.1% of Pipeline Slots
                    Decoder-0 Alone: 0.0% of Pipeline Slots
                Front-End Bandwidth DSB: 1.7% of Pipeline Slots
                (Info) DSB Coverage: 29.7%
                (Info) DSB Misses: 3.0% of Pipeline Slots
        Bad Speculation: 0.0% of Pipeline Slots
            Branch Mispredict: 0.1% of Pipeline Slots
                Other Mispredicts: 0.0% of Pipeline Slots
            Machine Clears: 0.0% of Pipeline Slots
                Other Nukes: 0.0% of Pipeline Slots
        Back-End Bound: 100.0% of Pipeline Slots
         | A significant portion of pipeline slots are remaining empty. When
         | operations take too long in the back-end, they introduce bubbles in
         | the pipeline that ultimately cause fewer pipeline slots containing
         | useful work to be retired per cycle than the machine is capable to
         | support. This opportunity cost results in slower execution. Long-
         | latency operations like divides and memory operations can cause this,
         | as can too many operations being directed to a single execution port
         | (for example, more multiply operations arriving in the back-end per
         | cycle than the execution unit can support).
         |
            Memory Bound: 59.5% of Pipeline Slots
             | The metric value is high. This can indicate that the significant
             | fraction of execution pipeline slots could be stalled due to
             | demand memory load and stores. Use Memory Access analysis to have
             | the metric breakdown by memory hierarchy, memory bandwidth
             | information, correlation by memory objects.
             |
                L1 Bound: 2.8% of Clockticks
                    DTLB Overhead: 1.6% of Clockticks
                        Load STLB Hit: 0.1% of Clockticks
                        Load STLB Miss: 1.5% of Clockticks
                    Loads Blocked by Store Forwarding: 0.0% of Clockticks
                    L1 Hit Latency: 0.6% of Clockticks
                    Lock Latency: 0.0% of Clockticks
                    Split Loads: 0.5% of Clockticks
                    FB Full: 21.7% of Clockticks
                L2 Bound: 0.1% of Clockticks
                L3 Bound: 8.2% of Clockticks
                 | This metric shows how often CPU was stalled on L3 cache, or
                 | contended with a sibling Core. Avoiding cache misses (L2
                 | misses/L3 hits) improves the latency and increases
                 | performance.
                 |
                    Contested Accesses: 0.1% of Clockticks
                    Data Sharing: 0.0% of Clockticks
                    L3 Latency: 0.2% of Clockticks
                    SQ Full: 14.4% of Clockticks
                     | This metric measures fraction of cycles where the Super
                     | Queue (SQ) was full taking into account all request-types
                     | and both hardware SMT threads. The Super Queue is used
                     | for requests to access the L2 cache or to go out to the
                     | Uncore.
                     |
                DRAM Bound: 24.5% of Clockticks
                 | This metric shows how often CPU was stalled on the main
                 | memory (DRAM). Caching typically improves the latency and
                 | increases performance.
                 |
                    Memory Bandwidth: 20.3% of Clockticks
                     | Issue: A significant fraction of cycles was stalled due
                     | to approaching bandwidth limits of the main memory
                     | (DRAM).
                     | 
                     | Tips: Improve data accesses to reduce cacheline transfers
                     | from/to memory using these possible techniques:
                     |     - Consume all bytes of each cacheline before it is
                     |       evicted (for example, reorder structure elements
                     |       and split non-hot ones).
                     |     - Merge compute-limited and bandwidth-limited loops.
                     |     - Use NUMA optimizations on a multi-socket system.
                     | 
                     | Note: software prefetches do not help a bandwidth-limited
                     | application.
                     |
                        MBA Stalls: 0.0% of Clockticks
                    Memory Latency: 4.3% of Clockticks
                        Local Memory: 1.3% of Clockticks
                        Remote Memory: 7.3% of Clockticks
                        Remote Cache: 0.6% of Clockticks
                Store Bound: 0.2% of Clockticks
                    Store Latency: 1.9% of Clockticks
                    False Sharing: 0.0% of Clockticks
                    Split Stores: 0.0%
                    Streaming Stores: 0.0% of Clockticks
                    DTLB Store Overhead: 0.1% of Clockticks
                        Store STLB Hit: 0.0% of Clockticks
                        Store STLB Miss: 0.0% of Clockticks
            Core Bound: 61.4% of Pipeline Slots
             | This metric represents how much Core non-memory issues were of a
             | bottleneck. Shortage in hardware compute resources, or
             | dependencies software's instructions are both categorized under
             | Core Bound. Hence it may indicate the machine ran out of an OOO
             | resources, certain execution units are overloaded or dependencies
             | in program's data- or instruction- flow are limiting the
             | performance (e.g. FP-chained long-latency arithmetic operations).
             |
                Divider: 0.0% of Clockticks
                Serializing Operations: 57.6% of Clockticks
                 | A significant fraction of cycles was spent handling
                 | serializing operations. Instructions like CPUID, WRMSR, or
                 | LFENCE serialize the out-of-order execution, which may limit
                 | performance.
                 |
                    Slow Pause: 46.9% of Clockticks
                     | A significant fraction of cycles was spent handling PAUSE
                     | inctructions.
                     |
                    C01 Wait: 0.0% of Clockticks
                    C02 Wait: 0.0% of Clockticks
                    Memory Fence: 0.0% of Clockticks
                AMX Busy: 0.0% of Clockticks
    Average CPU Frequency: 2.922 GHz
    Total Thread Count: 292
    Paused Time: 208.509s
Effective Physical Core Utilization: 1.4% (1.325 out of 96)
 | The metric value is low, which may signal a poor physical CPU cores
 | utilization caused by:
 |     - load imbalance
 |     - threading runtime overhead
 |     - contended synchronization
 |     - thread/process underutilization
 |     - incorrect affinity that utilizes logical cores instead of physical
 |       cores
 | Explore sub-metrics to estimate the efficiency of MPI and OpenMP parallelism
 | or run the Locks and Waits analysis to identify parallel bottlenecks for
 | other parallel runtimes.
 |
    Effective Logical Core Utilization: 0.9% (1.692 out of 192)
     | The metric value is low, which may signal a poor logical CPU cores
     | utilization. Consider improving physical core utilization as the first
     | step and then look at opportunities to utilize logical cores, which in
     | some cases can improve processor throughput and overall performance of
     | multi-threaded applications.
     |
Collection and Platform Info
    Application Command Line: python3 "/data/wangjiaqi/AMX/model_inference_amx_optimized.py" "--model_path" "/data/share/huggingface/hub/models--Qwen--Qwen3-32B/snapshots/9216db5781bf21249d130ec9da846c4624c16137" "--device" "cpu" "--benchmark_runs" "10" "--warmup_runs" "3" "--use_bf16" 
    Operating System: 6.8.0-79-generic DISTRIB_ID=Ubuntu DISTRIB_RELEASE=24.04 DISTRIB_CODENAME=noble DISTRIB_DESCRIPTION="Ubuntu 24.04.3 LTS"
    Computer Name: dasys-h200x8
    Result Size: 131.1 GB 
    Collection start time: 15:49:39 29/10/2025 UTC
    Collection stop time: 17:36:27 29/10/2025 UTC
    Collector Type: Driverless Perf system-wide sampling
    CPU
        Name: Intel(R) Xeon(R) Processor code named Emeraldrapids
        Frequency: 2.700 GHz
        Logical CPU Count: 192
        Max DRAM Single-Package Bandwidth: 362.000 GB/s
        LLC size: 272.6 MB 
        Cache Allocation Technology
            Level 2 capability: available
            Level 3 capability: available

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
